{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(\n",
    "       AutoModelWithLMHead,\n",
    "       AutoConfig,\n",
    "       Trainer,\n",
    "       AutoTokenizer,\n",
    "       TextDataset,\n",
    "       DataCollatorForLanguageModeling,\n",
    "       TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "       \"model_name\":\"gpt2\",\n",
    "       \"train_dataset_filename\":\"combined.txt\",\n",
    "       \"data_collator_block_size\":256,\n",
    "       \"output_dir\":\".\",\n",
    "       \"batch_size\":8,\n",
    "       \"epochs\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:966: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.10MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(config[\"model_name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(tokenizer = tokenizer, file_path = config[\"train_dataset_filename\"], block_size = config[\"data_collator_block_size\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "       output_dir=config[\"output_dir\"],\n",
    "       per_device_train_batch_size=config[\"batch_size\"],\n",
    "       num_train_epochs=config[\"epochs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "       model=model,\n",
    "       args=training_args,\n",
    "       data_collator=data_collator,\n",
    "       train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4286 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
